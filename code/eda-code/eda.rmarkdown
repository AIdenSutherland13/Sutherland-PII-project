---
title: "EDA"
date: "6-20-2024"
output: html_document
---

# Setup


```{r}
#load needed packages. make sure they are installed.
library(here) #for data loading/saving
library(dplyr)
library(skimr)
library(ggplot2)
```



Load the data.


```{r}
#Path to data. Note the use of the here() package and not absolute paths
load("~/GitHub/Sutherland-PII-project/data/raw-data/Data Cleaning/cleaned_data1.RData")
load("~/GitHub/Sutherland-PII-project/data/raw-data/Data Cleaning/cleaned_data2.RData")
load("~/GitHub/Sutherland-PII-project/data/raw-data/Data Cleaning/cleaned_data3.RData")
load("~/GitHub/Sutherland-PII-project/data/raw-data/Data Cleaning/cleaned_data4.RData")
load("~/GitHub/Sutherland-PII-project/data/raw-data/Data Cleaning/cleaned_games.RData")
```

```{r}
summary(games)
```

```{r}
str(games)
```


Here are some visualizations for each predictor variable for userscore


```{r}
# Boxplot of user scores by game classification
p1 <- ggplot(games, aes(x = class, y = userscore)) +
  geom_boxplot() +
  labs(title = "User Scores by Game Classification", x = "Game Classification", y = "User Score")
figure_file = here("results","figures","Class-user-box.png")
ggsave(filename = figure_file, plot = p1)

# Scatter plots of user scores vs. other numerical variables, colored by game classification
p2 <- ggplot(games, aes(x = average_playtime, y = userscore, color = class)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "User Scores vs. Average Playtime by Game Classification", x = "Average Playtime", y = "User Score")
figure_file2 = here("results","figures","average-user-scatter.png")
ggsave(filename = figure_file2, plot = p2)

p3 <- ggplot(games, aes(x = metascore.x, y = userscore, color = class)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "User Scores vs. Metascore by Game Classification", x = "Metascore", y = "User Score")
figure_file3 = here("results","figures","meta-user-scatter.png")
ggsave(filename = figure_file3, plot = p3)

p4 <- ggplot(games, aes(x = price, y = userscore, color = class)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "User Scores vs. Price by Game Classification", x = "Price", y = "User Score")
figure_file4 = here("results","figures","price-user-scatter.png")
ggsave(filename = figure_file4, plot = p4)


p5 <- ggplot(games, aes(x = owners, y = userscore, color = class)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "User Scores vs. Owners by Game Classification", x = "Owners", y = "User Score")
figure_file5 = here("results","figures","owners-user-scatter.png")
ggsave(filename = figure_file5, plot = p5)

p6 <- ggplot(games, aes(x = median_playtime, y = userscore, color = class)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "User Scores vs. Median Playtime by Game Classification", x = "Median Playtime", y = "User Score")
figure_file6 = here("results","figures","median-user-scatter.png")
ggsave(filename = figure_file6, plot = p6)
p1
p2
p3
p4
p5
p6
```




Here is a glm model with all the predictors including developer

```{r}
glm_model <- glm(userscore ~ class + average_playtime + metascore.x + price + owners + median_playtime + developer, 
                 data = games, 
                 family = gaussian())
summary(glm_model)
save(glm_model, file = 'first_model.RData')
```

There seem to be a few predictors that are statistically significant:
metascore.x
developerBohemia Interactive
developer Entertainment
developerHaemimont Games
developerSEGA 
developerTripwire Interactive




Below we check the model for non-linearity, homoscedasticity, normally distributed residuals, and identify if there are any influential data points.

```{r}
diagnostics<- par(mfrow = c(2, 2))
plot(glm_model)
par(mfrow = c(1, 1))
diagnostics
```



HERE IS WHERE I DID SUBMISSION 4, I AM COMPLETLY RESTRUCTURING THE REPOSITORY TO FIX THE PATHING ISSUES BUT THIS IS WHERE I WILL SHOW MY WORK FOR THE TIME BEING





# Split data into training and testing sets and train a linear regression model

```{r}
# Load Libraries
library(caret)
library(randomForest)
library(e1071)
library(ggplot2)



# Check for missing values
colSums(is.na(games))

# Handle missing values - choose one of the following methods
# Method 1: Remove rows with missing values
games <- na.omit(games)

# Method 2: Impute missing values
preProc <- preProcess(games, method = 'medianImpute')
games <- predict(preProc, games)

# Convert all character columns to factors
games[sapply(games, is.character)] <- lapply(games[sapply(games, is.character)], as.factor)

# Check levels of factor variables
factor_levels <- sapply(games, function(x) if(is.factor(x)) length(levels(x)) else NA)
print(factor_levels)

# Remove columns with only one level
single_level_cols <- names(factor_levels)[factor_levels <= 1]
games <- games[, !names(games) %in% single_level_cols]

# Split data into training and testing sets
set.seed(123)  # for reproducibility
trainIndex <- createDataPartition(games$userscore, p = 0.8, list = FALSE, times = 1)
gamesTrain <- games[trainIndex, ]
gamesTest <- games[-trainIndex, ]

# Train a linear regression model
model_lm <- train(userscore ~ ., data = gamesTrain, method = "lm")

# Make predictions
predictions_lm <- predict(model_lm, newdata = gamesTest)

# Evaluate the model
rmse_lm <- RMSE(predictions_lm, gamesTest$userscore)
mae_lm <- MAE(predictions_lm, gamesTest$userscore)

rmse_lm
mae_lm

```


RMSE (0.5475759): Indicates the average deviation of the predicted user scores from the actual user scores is about 0.548.
MAE (0.4164835): Indicates the average absolute error of the predicted user scores is about 0.416.


# Train a Random Forest model

```{r}
model_rf <- train(userscore ~ ., data = gamesTrain, method = "rf", trControl = trainControl(method = "cv", number = 10))

# Make predictions on the test set
predictions_rf <- predict(model_rf, newdata = gamesTest)

# Evaluate the model
rmse_rf <- RMSE(predictions_rf, gamesTest$userscore)
mae_rf <- MAE(predictions_rf, gamesTest$userscore)

# Print the evaluation metrics
print(paste("RMSE: ", rmse_rf))
print(paste("MAE: ", mae_rf))
```

The RMSE and MAE values from the Random Forest model (0.420 and 0.306, respectively) are slightly lower than those from the linear regression model (0.548 and 0.416, respectively). This indicates that the Random Forest model is providing slightly better predictions in this case.

Here I begin my cross validation


```{r}
# Load Libraries if not already loaded
library(caret)
library(randomForest)


# Check for missing values and handle them as needed
colSums(is.na(games))

# Handle missing values 
games <- na.omit(games)

# Convert all character columns to factors if needed
games[sapply(games, is.character)] <- lapply(games[sapply(games, is.character)], as.factor)

# Check levels of factor variables if needed
factor_levels <- sapply(games, function(x) if(is.factor(x)) length(levels(x)) else NA)
print(factor_levels)

# Remove columns with only one level if needed
single_level_cols <- names(factor_levels)[factor_levels <= 1]
games <- games[, !names(games) %in% single_level_cols]


# Define cross-validation control using 10-fold cross-validation
ctrl <- trainControl(method = "cv", number = 10)

# Train the Random Forest model with cross-validation
set.seed(123)  # for reproducibility
model_rf_cv <- train(userscore ~ ., data = gamesTrain, method = "rf", trControl = ctrl)

# Print the cross-validation results
print(model_rf_cv)

# Make predictions on the test set
predictions_rf_cv <- predict(model_rf_cv, newdata = gamesTest)

# Evaluate the model
rmse_rf_cv <- RMSE(predictions_rf_cv, gamesTest$userscore)
mae_rf_cv <- MAE(predictions_rf_cv, gamesTest$userscore)

# Print the evaluation metrics
print(paste("RMSE (Cross-validated): ", rmse_rf_cv))
print(paste("MAE (Cross-validated): ", mae_rf_cv))

```

