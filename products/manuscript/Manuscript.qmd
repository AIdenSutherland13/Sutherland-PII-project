---
title: "What Can Predict a Game's Userscore?"
subtitle: ""
author: Aiden Sutherland
date: "`r Sys.Date()`"
format:
  docx:
    toc: false
    number-sections: true
    highlight-style: github
bibliography: ../../assets/dataanalysis-references.bib
csl: ../../assets/apa.csl
editor: 
  markdown: 
    wrap: sentence
---

The structure below is one possible setup for a data analysis project (including the course project).
For a manuscript, adjust as needed.
You don't need to have exactly these sections, but the content covering those sections should be addressed.

This uses MS Word as output format.
[See here](https://quarto.org/docs/output-formats/ms-word.html) for more information.
You can switch to other formats, like html or pdf.
See [the Quarto documentation](https://quarto.org/) for other formats.

```{r, echo=FALSE, message=FALSE}
# load a few R packages
library(here)
library(knitr)
```

# SUBMISSION 1

In the past couple of years, the gaming market has been thrown for a loop with AAA titles (like Call of Duty) from these multi billion dollar game studios have absolutely flopped in comparison to some of these smaller and simpler games, a good example of one such that most people are aware of is Among Us, which got so popular that it made a small cringe cameo in the second 'Knives Out' movie.
An example of a flop from a game studio that almost killed the whole studio was the famous launch of Cyberpunk 2077.

Although I am finding trouble with finding data, my hope is to get the most popular AAA games and the most popular Indie games from the past 5 years(A total of 10 games) and do an analysis on how well these games performed in the market.
This data can hopefully be acquired from launcher stats (like steam or epic games) and news polls on twitter or other social media websites (like Reddit).

hypothesis/question I want answered: What effects a game's user score?
# Summary/Abstract *Write a summary of your project.*

{{< pagebreak >}}

# Introduction

## General Background Information

I have long asked myself what makes a good video game, and if certain video games are worth the prices they are being sold for.
There have been many instances where I spent a 60 dollars on a heaping pile of garbage, and then spent 10 dollars on some of the best fun I have had at a computer screen.
Is a game defined by the amount of replay-ability it has or the cost:quality ratio?
I wanted to find out what made people decide what made a game "good".

## Description of data and data source

The data I have collected stems from a few sources, one dataset is from a [tidy-tuesday exercise](https://github.com/rfordatascience/tidytuesday/blob/master/data/2019/2019-07-30/video_games.csv), one dataset from [kaggle](https://www.kaggle.com/datasets/destring/metacritic-reviewed-games-since-2000), and another I personally went through and gathered on the internet (the class-game.csv in the data folder.) This data holds a range of data going from overall metascore to median playtime on a video game while also including the development team and the publisher.

## Questions/Hypotheses to be addressed

What effects a game's user score and can the user score be predicted?

There was a project that was done with similar data that can be found clicking [here](https://towardsdatascience.com/analyzing-video-games-data-in-r-1afad7122aab) done by Hamza Rafiq, however I looked for data that added one more predictor that (given recent trends in the video game market) I was sure would be a predictor.

{{< pagebreak >}}

# Methods

*Describe your methods. That should describe the data, the cleaning processes, and the analysis approaches. You might want to provide a shorter description here and all the details in the supplement.* I found 2 different data sets that contained different kinds of sentiment about certain video game titles, 1 by metascore, and one by userscore.
I then combined them, found the top 25 games for userscore, amount of owners, median playtime, and average playtime then put them in the final dataset.
Overall there were 56 obs with 13 variables.
I later added another column with the game class (with 2 factors, one being "AAA" and the other being "indie")

## Data Acquisition

When it came to obtaining the data, the project mentioned before done by Hamza Rafiq was very helpful in obtaining the data for this project, however when it came to obtaining data for the game class variable, I had to manually search up the classes for each game after narrowing down the data to the top 25 games for each category.
The website that was integral in getting this data can be found [here](https://www.moddb.com/search?), just type in any game and all the info you would want will pop up.

## Data import and cleaning

In the Data folder of this repository, there are 2 data sets that are the ground work for the whole project.
One I imported from a different GitHub repo that can be found in the sources folder (I also downloaded it just in case) and the other is from Kaggle, which I downloaded directly from the site and imported it manually.
The cleaning process involved me joining the 2 data sets, making sure there were no duplicates of any kind in any variable (the publisher column required me to change anything that contained Warner Brothers to just Warner Brothers).
From then I broke down the final joined data set to the top 25 publishers and games based on user score and units sold.
I then joined my class_game.csv to the top 25 games and used that as the final dataset.

## Statistical analysis

*Explain anything related to your statistical analyses.*

For the statistical analysis, I split the cleaned_games.RData into training and test datasets, then trained and fit a random forest and linear regression model.

{{< pagebreak >}}

# Results

## Exploratory/Descriptive analysis

*Use a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.*

@fig-result shows a boxplot comparing game userscore by different game development classes.
There seems to be a slight trend leaning towards AAA games having a higher userscore.

```{r}
#| label: fig-result
#| fig-cap: "USerscore by game development class."
#| echo: FALSE
knitr::include_graphics(here("results","figures","Class-user-box.png"))
```

@fig-result2 shows a scatter plot showing average playtime compared to user score.
I separated the 2 different game classes by color given that at the beginning of this project I was so sure that this was going to be a predictor.
There seemed to be a clear negative correlation indicating the higher the score, the lower the average playtime.

```{r}
#| label: fig-result2
#| fig-cap: "USer Scores by Average playtime"
#| echo: FALSE
knitr::include_graphics(here("results","figures","average-user-scatter.png"))
```

Here is one more given that I have 6 and you get the idea, the final manuscript will have them all however I am running short on time atm.

@fig-result3 shows a scatterplot figure

```{r}
#| label: fig-result3
#| fig-cap: "MetaScore by UserScore"
#| echo: FALSE
knitr::include_graphics(here("results","figures","meta-user-scatter.png"))
```

@tbl-summarytable shows a summary of the data.

```{r}
load("~/GitHub/Sutherland-PII-project/code/eda-code/first_model.RData")
glm_summary <- summary(glm_model)

# Extract coefficients
coefficients <- glm_summary$coefficients

# Convert to a data frame for better manipulation
coeff_df <- as.data.frame(coefficients)

kable(coeff_df, digits = 4, caption = "Summary of GLM coefficients")

```

## Basic statistical analysis

Below are the statistically significant predictors from the glm model ran on the data.

```{r}
# Filter for statistically significant results (p-value < 0.05)
significant_coeff_df <- coeff_df[coeff_df$`Pr(>|t|)` < 0.05, ]
kable(significant_coeff_df, digits = 4, caption = "Statistically Significant Coefficients from GLM")
```

## Full analysis

*Use one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.*

```{r}
load("~/GitHub/Sutherland-PII-project/code/analysis-code/rf_metrics.RData")
load("~/GitHub/Sutherland-PII-project/code/analysis-code/rf_cv_metrics.RData")
load("~/GitHub/Sutherland-PII-project/code/analysis-code/lm_metrics.RData")
```

Linear Regression Model: RMSE (0.5475759): Indicates the average deviation of the predicted user scores from the actual user scores is about 0.548.
MAE (0.4164835): Indicates the average absolute error of the predicted user scores is about 0.416.

Random Forest Model: The RMSE and MAE values from the Random Forest model (0.420 and 0.306, respectively) are slightly lower than those from the linear regression model (0.548 and 0.416, respectively).
This indicates that the Random Forest model is providing slightly better predictions in this case.

RF Cross validation results: Random Forest

36 samples 11 predictors

No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 32, 32, 32, 33, 32, 33, ...
Resampling results across tuning parameters:

mtry RMSE Rsquared MAE\
2 0.7692740 0.4999631 0.6791293 99 0.5960730 0.7122966 0.4878380 196 0.6296674 0.6880695 0.5207912

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 99.
\[1\] "RMSE (Cross-validated): 0.412694747961779" \[1\] "MAE (Cross-validated): 0.302266190476191"

Example @tbl-resulttable2 shows a summary of a linear model fit.

```{r}
#| label: tbl-resulttable2
#| tbl-cap: "Linear model fit table."
#| echo: FALSE
resulttable2 = readRDS(here("results","tables","resulttable2.rds"))
knitr::kable(resulttable2)
```

{{< pagebreak >}}

# Discussion

## Summary and Interpretation

We applied Random Forest regression to predict user scores of games based on 11 predictor variables.
The model was trained and cross-validated using 10-fold cross-validation.
The optimal configuration, with mtry = 99, was selected based on minimizing the Root Mean Square Error (RMSE).
The cross-validated RMSE and Mean Absolute Error (MAE) were found to be 0.413 and 0.302, respectively, indicating that the model predicts user scores with an average deviation of approximately 0.413 units and an average absolute error of 0.302 units.
These results suggest that the Random Forest approach effectively captures the relationships between predictors and user scores, demonstrating promising performance in predicting game user scores.
*Summarize what you did, what you found and what it means.*

## Strengths and Limitations

*Discuss what you perceive as strengths and limitations of your analysis.*

## Conclusions

*What are the main take-home messages?*

*Include citations in your Rmd file using bibtex, the list of references will automatically be placed at the end*

This paper [@leek2015] discusses types of analyses.

These papers [@mckay2020; @mckay2020a] are good examples of papers published using a fully reproducible setup similar to the one shown in this template.

Note that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header.
Many more style files for almost any journal [are available](https://www.zotero.org/styles).
You also specify the location of your bibtex reference file in the YAML.
You can call your reference file anything you like.

{{< pagebreak >}}

# References
