---
title: "What Can Predict a Game's Userscore?"
subtitle: ""
author: Aiden Sutherland
date: "`r Sys.Date()`"
format:
  docx:
    toc: false
    number-sections: true
    highlight-style: github

editor: 
  markdown: 
    wrap: sentence
---

The structure below is one possible setup for a data analysis project (including the course project).
For a manuscript, adjust as needed.
You don't need to have exactly these sections, but the content covering those sections should be addressed.

This uses MS Word as output format.
[See here](https://quarto.org/docs/output-formats/ms-word.html) for more information.
You can switch to other formats, like html or pdf.
See [the Quarto documentation](https://quarto.org/) for other formats.

```{r, echo=FALSE, message=FALSE}
# load a few R packages
library(here)
library(knitr)
```


# Summary/Abstract *Write a summary of your project.*




{{< pagebreak >}}

# Introduction

## General Background Information

I have long asked myself what makes a good video game, and if certain video games are worth the prices they are being sold for.
There have been many instances where I spent a 60 dollars on a heaping pile of garbage, and then spent 10 dollars on some of the best fun I have had at a computer screen.
Is a game defined by the amount of replay-ability it has or the cost:quality ratio?
I wanted to find out what made people decide what made a game "good".

## Description of data and data source

The data I have collected stems from a few sources, one dataset is from a [tidy-tuesday exercise](https://github.com/rfordatascience/tidytuesday/blob/master/data/2019/2019-07-30/video_games.csv), one dataset from [kaggle](https://www.kaggle.com/datasets/destring/metacritic-reviewed-games-since-2000), and another I personally went through and gathered on the internet (the class-game.csv in the data folder.) This data holds a range of data going from overall metascore to median playtime on a video game while also including the development team and the publisher.

## Questions/Hypotheses to be addressed

What effects a game's user score and can the user score be predicted?

There was a project that was done with similar data that can be found clicking [here](https://towardsdatascience.com/analyzing-video-games-data-in-r-1afad7122aab) done by Hamza Rafiq, however I looked for data that added one more predictor that (given recent trends in the video game market) I was sure would be a predictor.

{{< pagebreak >}}

# Methods

*Describe your methods. That should describe the data, the cleaning processes, and the analysis approaches. You might want to provide a shorter description here and all the details in the supplement.* I found 2 different data sets that contained different kinds of sentiment about certain video game titles, 1 by metascore, and one by userscore.
I then combined them, found the top 25 games for userscore, amount of owners, median playtime, and average playtime then put them in the final dataset.
Overall there were 56 obs with 13 variables.
I later added another column with the game class (with 2 factors, one being "AAA" and the other being "indie")

## Data Acquisition

When it came to obtaining the data, the project mentioned before done by Hamza Rafiq was very helpful in obtaining the data for this project, however when it came to obtaining data for the game class variable, I had to manually search up the classes for each game after narrowing down the data to the top 25 games for each category.
The website that was integral in getting this data can be found [here](https://www.moddb.com/search?), just type in any game and all the info you would want will pop up.

## Data import and cleaning

In the Data folder of this repository, there are 2 data sets that are the ground work for the whole project.
One I imported from a different GitHub repo that can be found in the sources folder (I also downloaded it just in case) and the other is from Kaggle, which I downloaded directly from the site and imported it manually.
The cleaning process involved me joining the 2 data sets, making sure there were no duplicates of any kind in any variable (the publisher column required me to change anything that contained Warner Brothers to just Warner Brothers).
From then I broke down the final joined data set to the top 25 publishers and games based on user score and units sold.
I then joined my class_game.csv to the top 25 games and used that as the final dataset.

## Statistical analysis

*Explain anything related to your statistical analyses.*

For the statistical analysis, I split the cleaned_games.RData into training and test datasets, then trained and fit a random forest and linear regression model.

{{< pagebreak >}}

# Results

## Exploratory/Descriptive analysis

*Use a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.*

@fig-result shows a boxplot comparing game userscore by different game development classes.
There seems to be a slight trend leaning towards AAA games having a higher userscore.

```{r}
#| label: fig-result
#| fig-cap: "USerscore by game development class."
#| echo: FALSE
knitr::include_graphics(here("results","figures","Class-user-box.png"))
```

@fig-result2 shows a scatter plot showing average playtime compared to user score.
I separated the 2 different game classes by color given that at the beginning of this project I was so sure that this was going to be a predictor.
There seemed to be a clear negative correlation indicating the higher the score, the lower the average playtime.

```{r}
#| label: fig-result2
#| fig-cap: "USer Scores by Average playtime"
#| echo: FALSE
knitr::include_graphics(here("results","figures","average-user-scatter.png"))
```

@fig-result3 shows a scatterplot figure of user scores vs meta scores. There is a clear positive linear relationship between these 2 variables.

```{r}
#| label: fig-result3
#| fig-cap: "MetaScore by UserScore"
#| echo: FALSE
knitr::include_graphics(here("results","figures","meta-user-scatter.png"))
```
For more visualizations for other possible predictors, please refer to the eda.qmd file.



Here is a summary table of the predictors.

```{r}
load("~/GitHub/Sutherland-PII-project/code/eda-code/first_model.RData")
glm_summary <- summary(glm_model)

# Extract coefficients
coefficients <- glm_summary$coefficients

# Convert to a data frame for better manipulation
coeff_df <- as.data.frame(coefficients)

kable(coeff_df, digits = 4, caption = "Summary of GLM coefficients")

```

## Basic statistical analysis

Below are the statistically significant predictors from the glm model ran on the data (>.05).

```{r}
# Filter for statistically significant results (p-value < 0.05)
significant_coeff_df <- coeff_df[coeff_df$`Pr(>|t|)` < 0.05, ]
kable(significant_coeff_df, digits = 4, caption = "Statistically Significant Coefficients from GLM")
```

## Full analysis

*Use one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.*

```{r}
load("~/GitHub/Sutherland-PII-project/code/analysis-code/rf_metrics.RData")
load("~/GitHub/Sutherland-PII-project/code/analysis-code/rf_cv_metrics.RData")
load("~/GitHub/Sutherland-PII-project/code/analysis-code/lm_metrics.RData")
```

For the first model, after splitting the game data into test and training data, I ran 2 different models to see which one has better predicting power.

Below in the linear regression model for the training data, after training the model and making predictions, here are the MAE and RMSE statistics for the LM model as well as a plot for observed and predicted models
```{r}
print(paste("RMSE: ", rmse_lm ))
print(paste("MAE: ", mae_lm))
print(paste("R-squared: ", r_squared_lm))
```

```{r}
#| label: fig-result4
#| fig-cap: "LM model predictions Vs observed"
#| echo: FALSE
knitr::include_graphics(here("results","figures","lm_pred.png"))
```

Here is the Random Forest models MAE and RMSE statistic with the observed vs predicted plot.
```{r}
print(paste("RMSE: ", rmse_rf))
print(paste("MAE: ", mae_rf))
print(paste("R-squared: ", r_squared_rf))
```

```{r}
#| label: fig-result5
#| fig-cap: "RF model predictions Vs observed"
#| echo: FALSE
knitr::include_graphics(here("results","figures","rf_pred.png"))
```

Due to the RF model having a lower RMSE and MAE, while also having a higher r-squared, I proceeded to cross-validate the Random Forest model. Below is the results of my cross validation.



```{r}
print(paste("RMSE (Cross-validated): ", rmse_rf_cv))
print(paste("MAE (Cross-validated): ", mae_rf_cv))
print(paste("R-squared: ", r_squared_rf_cv))
```
```{r}
#| label: fig-result6
#| fig-cap: "RF_CV model predictions Vs observed"
#| echo: FALSE
knitr::include_graphics(here("results","figures","rf_cv_pred.png"))
```
After cross validating the Random Forest model, the r-squared value slightly increases while the MAE and RMSE slightly decreases, so the cross validated model is somewhat better than the RF model.



{{< pagebreak >}}

# Discussion

## Summary and Interpretation

We applied Random Forest regression to predict user scores of games based on 11 predictor variables.
The model was trained and cross-validated using 10-fold cross-validation.
The optimal configuration, with mtry = 82, was selected based on minimizing the Root Mean Square Error (RMSE).
The cross-validated RMSE and Mean Absolute Error (MAE) were found to be 0.3968 and 0.2896, respectively, indicating that the model predicts user scores with an average deviation of approximately 0.3968 units and an average absolute error of 0.2896 units.
These results suggest that the Random Forest approach effectively captures the relationships between predictors and user scores, demonstrating promising performance in predicting game user scores.


## Strengths and Limitations

One extreme limitation to this project was the fact that I only had the top 25 game classes. Provided I had the time (between classes and work) to match all of the games mentioned with its respective class I feel that the models would fit a lot better. One strength I could always rely on was how detailed the data could get. Depending on how you clean/wrangle it you could use the data for a multitude of projects that involve different hypothesis. Again, i belive having more data to go through in this project could conclude more concrete models for prediction.
*Discuss what you perceive as strengths and limitations of your analysis.*

## Conclusions

*What are the main take-home messages?*
Given the r-squared value of the cross validated Random Forest Model, we can conclude that we can roughly conclude the userscore from the 11 predictor variables in the games.csv file. Some of the predictors (like owners and metascore) would need to be predicted if you were to want to predict the userscore of an unreleased game. Overall this project was really fun due to it pertaining to a topic I find truly fascinating being a video game consumer myself. In the future I would probably look for another scoring metrics besides metascore and userscore in order to broaden consumer sentiment.
*Include citations in your Rmd file using bibtex, the list of references will automatically be placed at the end*


{{< pagebreak >}}

# References

tidy-tuesday exercise - (https://github.com/rfordatascience/tidytuesday/blob/master/data/2019/2019-07-30/video_games.csv)
kaggle Metacritic games - (https://www.kaggle.com/datasets/destring/metacritic-reviewed-games-since-2000)
Analyzing Video Games Data in R, Hamza Rafiq - https://towardsdatascience.com/analyzing-video-games-data-in-r-1afad7122aab





